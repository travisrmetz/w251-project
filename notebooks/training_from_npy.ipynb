{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImJOhE4BrfK7"
   },
   "source": [
    "# Training the model from npy files\n",
    "---\n",
    "\n",
    "This study uses nearly 7,000 images images in the area bounded by 36N to 40N and 074W to 078W and 2020-05-25T22:00:00 to 2020-05-26T02:00:00.  Images are compressed to 224 x 224 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlNrYnNsrp4C"
   },
   "source": [
    "## Import useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Y0LD8oscOFG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Dropout, Lambda, Concatenate, Flatten, concatenate\n",
    "from tensorflow.keras import Model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import ktrain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zheMHJQI0M6R"
   },
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HchIutGk3xoe"
   },
   "outputs": [],
   "source": [
    "def haversine_loss(y_true, y_pred, denorm=(36.0, 4.0, -78.0, 4.0), R=3443.92):\n",
    "    \"\"\"\n",
    "    Returns the mean squared haversine distance\n",
    "    between arrays consisting of lattitudes and\n",
    "    longitudes.\n",
    "    \n",
    "    Args:\n",
    "        y_true:  Either an np.array or a tf.constant\n",
    "                 of dimensions m x 2 where m in the\n",
    "                 number of observations.  Each row is\n",
    "                 an ordered pair of [lat, long].\n",
    "                 \n",
    "        y_pred:  Has the same form as y_true.\n",
    "        \n",
    "        dnorm:   A tuple of four values needed to\n",
    "                 convert normalized lat and long back\n",
    "                 to actual values.\n",
    "        \n",
    "        R:       Float giving the radius of the earth.\n",
    "                 The default value is in nautical\n",
    "                 miles.  Values in other units:\n",
    "                 \n",
    "                 kilometers    -> 6378.14\n",
    "                 statute miles -> 3963.19\n",
    "                 smoots        -> 3.748e+6\n",
    "        \n",
    "    Returns:\n",
    "        tf.tensor of shape () and dtype float64 giving\n",
    "        the mean square distance error using the\n",
    "        haversine function.\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "        Input:\n",
    "        y1     = np.array([[0, 0]])\n",
    "        y_hat1 = np.array([[0, 180]])\n",
    "        \n",
    "        Expected result:\n",
    "        (pi * R) ** 2 = 117059281.6 nm^2\n",
    "        \n",
    "        Input:\n",
    "        y2     = np.array([[0, 0]])\n",
    "        y_hat2 = np.array([[90, 0]])\n",
    "        \n",
    "        Expected result:\n",
    "        (pi * R / 2) ** 2 = 29264820.4 nm^2\n",
    "        \n",
    "        Input:\n",
    "        Portmsouth, VA to Rota, Spain\n",
    "        y3     = tf.constant([[36.8354, -76.2983]])\n",
    "        y_hat3 = tf.constant([[36.6237, -6.3601]])\n",
    "        \n",
    "        Expected result:\n",
    "        37065212.0 km^2\n",
    "        \n",
    "    Notes:\n",
    "        Closely follows the JS implmentation at\n",
    "        https://www.movable-type.co.uk/scripts/latlong.html.\n",
    "    \"\"\"\n",
    "    # Break inputs into lattitudes and longitudes for\n",
    "    # convienience\n",
    "\n",
    "    # Convert normalized lat and long into actuals\n",
    "    lat_min, lat_range, long_min, long_range = denorm\n",
    "    lat1  = y_true[:,0] * lat_range + lat_min\n",
    "    lat2  = y_pred[:,0] * lat_range + lat_min\n",
    "    long1 = y_true[:,1] * long_range + long_min\n",
    "    long2 = y_pred[:,1] * long_range + long_min\n",
    "    \n",
    "    # Compute phis and lambdas \n",
    "    phi1 = lat1 * np.pi / 180\n",
    "    phi2 = lat2 * np.pi / 180\n",
    "    delta_phi    = (lat2 - lat1) * np.pi / 180\n",
    "    delta_lambda = (long2 - long1) * np.pi / 180\n",
    "    \n",
    "    # Intermediate computations\n",
    "    a = tf.square(tf.sin(delta_phi / 2)) + tf.cos(phi1) * tf.cos(phi2) * tf.square(tf.sin(delta_lambda / 2))\n",
    "    c = 2 * tf.atan2(tf.sqrt(a), tf.sqrt(1 - a))\n",
    "    \n",
    "    # Compute distances\n",
    "    d = R * c\n",
    "    \n",
    "    # Compute the mean squared distance (MSE)\n",
    "    return tf.reduce_mean(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10QkIM_Rso10"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HkcP-D-Esrce"
   },
   "source": [
    "### Load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train_big.npy')\n",
    "x_test = np.load('x_test_big.npy')\n",
    "y_train = np.load('y_train_big.npy')\n",
    "y_test = np.load('y_test_big.npy')\n",
    "t_train = np.load('t_train_big.npy')\n",
    "t_test = np.load('t_test_big.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0196078431372549"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6788, 224, 224, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6788, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5012638402133798"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0196078431372549"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CNN model with multiple inputs\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Input(shape=x_train[0].shape)\n",
    "input_time = Input(shape=t_train[0].shape)\n",
    "i = Conv2D(filters=5, kernel_size=10, padding='same', activation='relu')(input_image)\n",
    "i = Conv2D(filters=1, kernel_size=10, padding='same', activation='relu')(i)\n",
    "i = Flatten()(i)\n",
    "t = Flatten()(input_time)\n",
    "ti = concatenate([i, t])\n",
    "ti = Dense(256, activation='relu')(ti)\n",
    "ti = Dropout(0.2)(ti)\n",
    "outputs = Dense(2, activation='sigmoid')(ti)\n",
    "#outputs = Dense(2)(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_image, input_time], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 224, 224, 5)  505         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 224, 1)  501         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 50176)        0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1)            0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 50177)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          12845568    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            514         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,847,088\n",
      "Trainable params: 12,847,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss=haversine_loss,\n",
    "             metrics=[haversine_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=([x_train, t_train], y_train),\n",
    "                             val_data=([x_test, t_test], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.0002...\n",
      "Train on 6788 samples, validate on 715 samples\n",
      "Epoch 1/1024\n",
      "6788/6788 [==============================] - 6s 949us/sample - loss: 51.2657 - haversine_loss: 51.2155 - val_loss: 32.9508 - val_haversine_loss: 32.9832\n",
      "Epoch 2/1024\n",
      "6788/6788 [==============================] - 5s 729us/sample - loss: 31.9324 - haversine_loss: 31.9065 - val_loss: 27.0002 - val_haversine_loss: 27.0177\n",
      "Epoch 3/1024\n",
      "6788/6788 [==============================] - 4s 649us/sample - loss: 26.8124 - haversine_loss: 26.7983 - val_loss: 23.9992 - val_haversine_loss: 24.0582\n",
      "Epoch 4/1024\n",
      "6788/6788 [==============================] - 4s 659us/sample - loss: 22.9898 - haversine_loss: 23.0064 - val_loss: 21.2420 - val_haversine_loss: 21.2923\n",
      "Epoch 5/1024\n",
      "6788/6788 [==============================] - 4s 661us/sample - loss: 20.2086 - haversine_loss: 20.2194 - val_loss: 18.9528 - val_haversine_loss: 19.0270\n",
      "Epoch 6/1024\n",
      "6788/6788 [==============================] - 4s 657us/sample - loss: 17.5021 - haversine_loss: 17.4699 - val_loss: 16.7754 - val_haversine_loss: 16.8699\n",
      "Epoch 7/1024\n",
      "6788/6788 [==============================] - 4s 645us/sample - loss: 15.6628 - haversine_loss: 15.6489 - val_loss: 14.8227 - val_haversine_loss: 14.9415\n",
      "Epoch 8/1024\n",
      "6788/6788 [==============================] - 4s 645us/sample - loss: 14.0305 - haversine_loss: 13.9932 - val_loss: 13.9413 - val_haversine_loss: 14.0786\n",
      "Epoch 9/1024\n",
      "6788/6788 [==============================] - 4s 654us/sample - loss: 12.9221 - haversine_loss: 12.9153 - val_loss: 12.3327 - val_haversine_loss: 12.3881\n",
      "Epoch 10/1024\n",
      "6788/6788 [==============================] - 4s 646us/sample - loss: 11.8664 - haversine_loss: 11.8876 - val_loss: 11.6623 - val_haversine_loss: 11.6766\n",
      "Epoch 11/1024\n",
      "6788/6788 [==============================] - 4s 652us/sample - loss: 11.1036 - haversine_loss: 11.0861 - val_loss: 10.8658 - val_haversine_loss: 10.8629\n",
      "Epoch 12/1024\n",
      "6788/6788 [==============================] - 4s 651us/sample - loss: 10.6268 - haversine_loss: 10.6212 - val_loss: 10.6574 - val_haversine_loss: 10.6898\n",
      "Epoch 13/1024\n",
      "6788/6788 [==============================] - 4s 655us/sample - loss: 10.2306 - haversine_loss: 10.2470 - val_loss: 10.1328 - val_haversine_loss: 10.1053\n",
      "Epoch 14/1024\n",
      "6788/6788 [==============================] - 4s 647us/sample - loss: 9.6905 - haversine_loss: 9.6952 - val_loss: 9.7022 - val_haversine_loss: 9.7135\n",
      "Epoch 15/1024\n",
      "6788/6788 [==============================] - 4s 658us/sample - loss: 9.3517 - haversine_loss: 9.3438 - val_loss: 9.3005 - val_haversine_loss: 9.3343\n",
      "Epoch 16/1024\n",
      "6788/6788 [==============================] - 4s 657us/sample - loss: 9.1160 - haversine_loss: 9.1048 - val_loss: 8.9259 - val_haversine_loss: 8.9224\n",
      "Epoch 17/1024\n",
      "6788/6788 [==============================] - 4s 663us/sample - loss: 8.8471 - haversine_loss: 8.8539 - val_loss: 8.7987 - val_haversine_loss: 8.8104\n",
      "Epoch 18/1024\n",
      "6788/6788 [==============================] - 4s 648us/sample - loss: 8.5764 - haversine_loss: 8.5659 - val_loss: 8.6572 - val_haversine_loss: 8.7112\n",
      "Epoch 19/1024\n",
      "6788/6788 [==============================] - 4s 650us/sample - loss: 8.4458 - haversine_loss: 8.4433 - val_loss: 8.5473 - val_haversine_loss: 8.5901\n",
      "Epoch 20/1024\n",
      "6788/6788 [==============================] - 5s 664us/sample - loss: 8.1381 - haversine_loss: 8.1410 - val_loss: 8.2282 - val_haversine_loss: 8.2354\n",
      "Epoch 21/1024\n",
      "6788/6788 [==============================] - 4s 643us/sample - loss: 8.0327 - haversine_loss: 8.0474 - val_loss: 8.2686 - val_haversine_loss: 8.2954\n",
      "Epoch 22/1024\n",
      "6788/6788 [==============================] - 5s 668us/sample - loss: 7.8739 - haversine_loss: 7.8889 - val_loss: 8.0092 - val_haversine_loss: 8.0131\n",
      "Epoch 23/1024\n",
      "6788/6788 [==============================] - 4s 661us/sample - loss: 7.6912 - haversine_loss: 7.7025 - val_loss: 7.9704 - val_haversine_loss: 7.9938\n",
      "Epoch 24/1024\n",
      "6788/6788 [==============================] - 5s 666us/sample - loss: 7.6967 - haversine_loss: 7.6876 - val_loss: 7.8462 - val_haversine_loss: 7.8677\n",
      "Epoch 25/1024\n",
      "6788/6788 [==============================] - 5s 667us/sample - loss: 7.7151 - haversine_loss: 7.7124 - val_loss: 7.7722 - val_haversine_loss: 7.7661\n",
      "Epoch 26/1024\n",
      "6788/6788 [==============================] - 5s 666us/sample - loss: 7.5301 - haversine_loss: 7.5400 - val_loss: 7.6681 - val_haversine_loss: 7.7085\n",
      "Epoch 27/1024\n",
      "6788/6788 [==============================] - 4s 650us/sample - loss: 7.3997 - haversine_loss: 7.4035 - val_loss: 7.4557 - val_haversine_loss: 7.4469\n",
      "Epoch 28/1024\n",
      "6788/6788 [==============================] - 4s 660us/sample - loss: 7.2560 - haversine_loss: 7.2685 - val_loss: 7.3873 - val_haversine_loss: 7.4200\n",
      "Epoch 29/1024\n",
      "6788/6788 [==============================] - 4s 644us/sample - loss: 7.1733 - haversine_loss: 7.1715 - val_loss: 7.4166 - val_haversine_loss: 7.4224\n",
      "Epoch 30/1024\n",
      "6752/6788 [============================>.] - ETA: 0s - loss: 7.1299 - haversine_loss: 7.1299\n",
      "Epoch 00030: Reducing Max LR on Plateau: new max lr will be 0.0001 (if not early_stopping).\n",
      "6788/6788 [==============================] - 4s 642us/sample - loss: 7.1281 - haversine_loss: 7.1388 - val_loss: 7.4121 - val_haversine_loss: 7.4355\n",
      "Epoch 31/1024\n",
      "6788/6788 [==============================] - 5s 665us/sample - loss: 6.8051 - haversine_loss: 6.7961 - val_loss: 6.8287 - val_haversine_loss: 6.8746\n",
      "Epoch 32/1024\n",
      "6788/6788 [==============================] - 4s 646us/sample - loss: 6.5418 - haversine_loss: 6.5264 - val_loss: 7.0189 - val_haversine_loss: 7.0643\n",
      "Epoch 33/1024\n",
      "6752/6788 [============================>.] - ETA: 0s - loss: 6.4754 - haversine_loss: 6.4754\n",
      "Epoch 00033: Reducing Max LR on Plateau: new max lr will be 5e-05 (if not early_stopping).\n",
      "6788/6788 [==============================] - 4s 638us/sample - loss: 6.4778 - haversine_loss: 6.4819 - val_loss: 6.8999 - val_haversine_loss: 6.9612\n",
      "Epoch 34/1024\n",
      "6788/6788 [==============================] - 4s 661us/sample - loss: 6.2523 - haversine_loss: 6.2463 - val_loss: 6.8268 - val_haversine_loss: 6.8428\n",
      "Epoch 35/1024\n",
      "6788/6788 [==============================] - 4s 663us/sample - loss: 6.1309 - haversine_loss: 6.1204 - val_loss: 6.6776 - val_haversine_loss: 6.6933\n",
      "Epoch 36/1024\n",
      "6788/6788 [==============================] - 4s 645us/sample - loss: 6.1779 - haversine_loss: 6.1786 - val_loss: 6.6846 - val_haversine_loss: 6.7054\n",
      "Epoch 37/1024\n",
      "6788/6788 [==============================] - 4s 663us/sample - loss: 6.0294 - haversine_loss: 6.0245 - val_loss: 6.6702 - val_haversine_loss: 6.7061\n",
      "Epoch 38/1024\n",
      "6788/6788 [==============================] - 5s 663us/sample - loss: 6.0199 - haversine_loss: 6.0238 - val_loss: 6.6630 - val_haversine_loss: 6.6837\n",
      "Epoch 39/1024\n",
      "6788/6788 [==============================] - 4s 644us/sample - loss: 5.9668 - haversine_loss: 5.9794 - val_loss: 6.6668 - val_haversine_loss: 6.6989\n",
      "Epoch 40/1024\n",
      "6752/6788 [============================>.] - ETA: 0s - loss: 5.9568 - haversine_loss: 5.9568\n",
      "Epoch 00040: Reducing Max LR on Plateau: new max lr will be 2.5e-05 (if not early_stopping).\n",
      "6788/6788 [==============================] - 4s 641us/sample - loss: 5.9539 - haversine_loss: 5.9527 - val_loss: 6.8078 - val_haversine_loss: 6.8424\n",
      "Epoch 41/1024\n",
      "6788/6788 [==============================] - 4s 648us/sample - loss: 5.8377 - haversine_loss: 5.8298 - val_loss: 6.6126 - val_haversine_loss: 6.6337\n",
      "Epoch 42/1024\n",
      "6788/6788 [==============================] - 4s 653us/sample - loss: 5.7697 - haversine_loss: 5.7682 - val_loss: 6.5650 - val_haversine_loss: 6.5926\n",
      "Epoch 43/1024\n",
      "6788/6788 [==============================] - 4s 639us/sample - loss: 5.8065 - haversine_loss: 5.7946 - val_loss: 6.6792 - val_haversine_loss: 6.7133\n",
      "Epoch 44/1024\n",
      "6752/6788 [============================>.] - ETA: 0s - loss: 5.8144 - haversine_loss: 5.8144\n",
      "Epoch 00044: Reducing Max LR on Plateau: new max lr will be 1.25e-05 (if not early_stopping).\n",
      "6788/6788 [==============================] - 4s 629us/sample - loss: 5.8082 - haversine_loss: 5.8051 - val_loss: 6.6080 - val_haversine_loss: 6.6414\n",
      "Epoch 45/1024\n",
      "6788/6788 [==============================] - 4s 630us/sample - loss: 5.6951 - haversine_loss: 5.6894 - val_loss: 6.5918 - val_haversine_loss: 6.6295\n",
      "Epoch 46/1024\n",
      "6788/6788 [==============================] - 4s 649us/sample - loss: 5.7265 - haversine_loss: 5.7189 - val_loss: 6.5449 - val_haversine_loss: 6.6002\n",
      "Epoch 47/1024\n",
      "6788/6788 [==============================] - 4s 633us/sample - loss: 5.6037 - haversine_loss: 5.6108 - val_loss: 6.5541 - val_haversine_loss: 6.5982\n",
      "Epoch 48/1024\n",
      "6788/6788 [==============================] - 4s 656us/sample - loss: 5.6260 - haversine_loss: 5.6187 - val_loss: 6.5268 - val_haversine_loss: 6.5635\n",
      "Epoch 49/1024\n",
      "6788/6788 [==============================] - 4s 660us/sample - loss: 5.6584 - haversine_loss: 5.6514 - val_loss: 6.5100 - val_haversine_loss: 6.5586\n",
      "Epoch 50/1024\n",
      "6788/6788 [==============================] - 4s 652us/sample - loss: 5.6198 - haversine_loss: 5.6215 - val_loss: 6.4444 - val_haversine_loss: 6.4882\n",
      "Epoch 51/1024\n",
      "6788/6788 [==============================] - 4s 638us/sample - loss: 5.6600 - haversine_loss: 5.6506 - val_loss: 6.4901 - val_haversine_loss: 6.5340\n",
      "Epoch 52/1024\n",
      "6752/6788 [============================>.] - ETA: 0s - loss: 5.5908 - haversine_loss: 5.5908\n",
      "Epoch 00052: Reducing Max LR on Plateau: new max lr will be 6.25e-06 (if not early_stopping).\n",
      "6788/6788 [==============================] - 4s 640us/sample - loss: 5.5908 - haversine_loss: 5.5908 - val_loss: 6.5266 - val_haversine_loss: 6.5690\n",
      "Epoch 53/1024\n",
      "6788/6788 [==============================] - 4s 645us/sample - loss: 5.6220 - haversine_loss: 5.6141 - val_loss: 6.5066 - val_haversine_loss: 6.5474\n",
      "Epoch 54/1024\n",
      "6752/6788 [============================>.] - ETA: 0s - loss: 5.5616 - haversine_loss: 5.5616\n",
      "Epoch 00054: Reducing Max LR on Plateau: new max lr will be 3.125e-06 (if not early_stopping).\n",
      "6788/6788 [==============================] - 4s 643us/sample - loss: 5.5616 - haversine_loss: 5.5642 - val_loss: 6.4809 - val_haversine_loss: 6.5153\n",
      "Epoch 55/1024\n",
      "6752/6788 [============================>.] - ETA: 0s - loss: 5.5991 - haversine_loss: 5.5991Restoring model weights from the end of the best epoch.\n",
      "6788/6788 [==============================] - 4s 658us/sample - loss: 5.5930 - haversine_loss: 5.5912 - val_loss: 6.4939 - val_haversine_loss: 6.5413\n",
      "Epoch 00055: early stopping\n",
      "Weights from best epoch have been loaded into model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb9f00560b8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.autofit(2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirming validation loss and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = learner.model.predict([x_test, t_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.444400189148059"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haversine_loss(y_test, y_hat.astype('double')).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.save('../data/models/model_for_travis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "working_copy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
